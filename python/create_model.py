import pandas as pd
import xgboost as xgb
import sklearn
import argparse

parser = argparse.ArgumentParser(description='Train model for cryptocurrency.')
parser.add_argument("-s", help="symbol of currency", type=str)

args = parser.parse_args()
symbol = args.s.strip()

short_window_size = 2
long_window_size = 5


def get_predictor(data):
    predictors = pd.DataFrame({"open_sma": data.open.rolling(window=short_window_size).mean(),
                               "open_smal": data.open.rolling(window=long_window_size).mean(),

                               "low_sma": data.low.rolling(window=short_window_size).mean(),
                               "low_smal": data.low.rolling(window=long_window_size).mean(),

                               "close_sma": data.close.rolling(window=short_window_size).mean(),
                               "close_smal": data.close.rolling(window=long_window_size).mean(),

                               "high_sma": data.high.rolling(window=short_window_size).mean(),
                               "high_smal": data.high.rolling(window=long_window_size).mean(),

                               "spread_sma": data.spread.rolling(window=short_window_size).mean()})

    # additional features
    #predictors["spread_rel"] = data.spread_sma / data.open_sma + 0.0001  # avoid null division
    predictors["volume"] = data.volume + 0.001  # avoid null division

    # The rows with nulls generated by rolling values will be removed.
    predictors = predictors.dropna()

    return predictors


def get_x_and_y(p):
    target = pd.DataFrame({"low_value": ((p.low_sma.shift(-1) - p.low_sma) + (p.low_smal.shift(-1) - p.low_smal)) / 2,
                           "open_value": ((p.open_sma.shift(-1) - p.open_sma) + (p.open_smal.shift(-1) - p.open_smal)) / 2,
                           "close_value": ((p.close_sma.shift(-1) - p.close_sma) + (p.close_smal.shift(-1) - p.close_smal)) / 2,
                           "high_value": ((p.high_sma.shift(-1) - p.high_sma) + (p.high_smal.shift(-1) - p.high_smal)) / 2,
                           "relspread": ((p.spread_sma / (p.open_sma + 0.0001)) / (p.spread_sma.shift(-1) / (p.open_sma.shift(-1) + 0.0001))),
                           "difvolume": (p.volume / p.volume.shift(-1))}).dropna()

    X = pd.merge(p, target, left_index=True, right_index=True)[p.columns]
    y = pd.merge(p, target, left_index=True, right_index=True)[target.columns]

    return X, y


MAIN_DATA = pd.read_csv("data/data-" + symbol + ".csv", sep=",", header=0, decimal='.')
MAIN_DATA = MAIN_DATA.set_index("date")

predictor = get_predictor(MAIN_DATA)
X, y = get_x_and_y(predictor)

train_samples = int(X.shape[0] * 1)
last_10_entries = int(X.shape[0] - 10)

X_train = X.iloc[:last_10_entries]
y_train = y.iloc[:last_10_entries]

#regressorLow = xgb.XGBRegressor(gamma=0.0, n_estimators=100, base_score=0.7, colsample_bytree=1, learning_rate=0.01)
regressorLow = xgb.XGBRegressor(gamma=0.0, n_estimators=500, base_score=0.7, colsample_bytree=1, learning_rate=0.2, max_depth=9)
xgbModelLow = regressorLow.fit(X_train, y_train.low_value)

regressorHigh = xgb.XGBRegressor(gamma=0.0, n_estimators=500, base_score=0.7, colsample_bytree=1, learning_rate=0.2, max_depth=9)
xgbModelHigh = regressorLow.fit(X_train, y_train.high_value)

regressorOpen = xgb.XGBRegressor(gamma=0.0, n_estimators=500, base_score=0.7, colsample_bytree=1, learning_rate=0.2, max_depth=9)
xgbModelOpen = regressorLow.fit(X_train, y_train.open_value)

regressorClose = xgb.XGBRegressor(gamma=0.0, n_estimators=500, base_score=0.7, colsample_bytree=1, learning_rate=0.2, max_depth=9)
xgbModelClose = regressorLow.fit(X_train, y_train.close_value)

regressorSpread = xgb.XGBRegressor(gamma=0.0, n_estimators=100, base_score=0.7, colsample_bytree=1, learning_rate=0.2, max_depth=9)
xgbModelSpread = regressorSpread.fit(X_train, y_train.relspread)

#regressorVolume = xgb.XGBRegressor(gamma=0.0, n_estimators=20, base_score=0.7, colsample_bytree=1, learning_rate=0.01)
#xgbModelVolume = regressorVolume.fit(X_train, y_train.difvolume)

n = len(MAIN_DATA) - (long_window_size + 2)
last_set = MAIN_DATA[n:]
org_length = len(last_set)
i = -1

for y in range(25):
    X_test, y_test = get_x_and_y(get_predictor(last_set))
    if not X_test.empty:
        i = len(last_set) - 1
        last_entry = last_set[i:].iloc[0]

        new_rel_spread = 1
        if last_entry.low != 0:
            new_rel_spread = (xgbModelSpread.predict(X_test)[-1] * (last_entry.spread / last_entry.low))

        new_low = max(last_entry.low - xgbModelLow.predict(X_test)[-1], 0.0)
        new_spread = new_rel_spread * new_low

        # avoid overspread
        if new_spread > new_low:
            new_spread = new_low / 2

        new_open = max(last_entry.open - xgbModelOpen.predict(X_test)[-1], 0.0)
        new_close = max(last_entry.close - xgbModelClose.predict(X_test)[-1], 0.0)

        #new_volume = round(last_entry.volume * xgbModelVolume.predict(X_test)[-1])
        new_volume = last_entry.volume

        new_high = max(((last_entry.low + new_spread) + (last_entry.high - xgbModelHigh.predict(X_test)[-1])) / 2, 0.0)
        new_low, new_high = sorted([new_low, new_high]) # ensure low is smaller high
        new_spread = new_high - new_low

        last_set = last_set.append(pd.Series({
            "open": new_open,
            "close": new_close,
            "high": new_high,
            "low": new_low,
            "spread": new_spread,
            "volume": new_volume
        }, name=last_set[i:].index.values[0] + 86400000), ignore_index=False)

if i != -1:
    last_set[org_length:i].to_csv("data/predict-low-high-" + symbol + ".csv", sep=",", header=True, decimal='.')
